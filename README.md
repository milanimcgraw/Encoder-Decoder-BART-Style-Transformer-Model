# ğŸ¤– Creating and Training a Encoder-Decoder Style Model (from Attention is All You Need) From Scratch 

In this repo, we'll be working through an example of how we can create, and then train, the original Transformer from the Attention is All You Need paper. This project was done during Week 1 of AI Makerspace's LLM Engineering Cohort 2 (LLME2).

### âš™ï¸The colab link to the code is found and (will also be included in this repo) [here.](https://colab.research.google.com/drive/1Gwaz3slF8Jx5zlkW5_RK3bD43kFchVSX?usp=sharing)

### ğŸ«‚The video walkthrough can be found [here.](https://www.loom.com/share/883c58fce40241999ab5b6c716c24135?sid=3d9a1279-65ed-4bee-b1d5-2cbc32ef53d1)

# âš™ï¸The Build Process

### Build the Model
Build the major components of an encoder/decoder style transformer network from scratch using PyTorch.

### Train the Model

Train our new network on a toy dataset to showcase how the training loop works and how we pass data through our network.
